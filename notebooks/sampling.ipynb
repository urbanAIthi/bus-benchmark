{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = '2022-01-01'\n",
    "END_DATE = '2024-12-31'\n",
    "\n",
    "COVERAGE_FREQ = 'W'\n",
    "COVERAGE_FREQ_STRFTIME = '%Y-%W'\n",
    "\n",
    "TRIP_COUNT_THRESHOLD = 1000\n",
    "LINK_COUNT_THRESHOLD = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(path):\n",
    "    name, _ = os.path.splitext(os.path.basename(path))\n",
    "    return name\n",
    "\n",
    "def iso_weeks_in_year(yr: int) -> int:\n",
    "    \"\"\"\n",
    "    Number of ISO weeks in calendar year `yr`.\n",
    "    Dec 31 can fall in week 1, so we treat that as 52.\n",
    "    \"\"\"\n",
    "    w = date(yr, 12, 31).isocalendar()[1]\n",
    "    return 52 if w == 1 else w\n",
    "\n",
    "def summarize_trips(path: str) -> pd.DataFrame:\n",
    "    df_uncleaned = pd.read_parquet(path)\n",
    "    df_uncleaned['has_geometry'] = df_uncleaned['from_geometry'].notna() & df_uncleaned['to_geometry'].notna()\n",
    "\n",
    "    df = df_uncleaned[df_uncleaned['valid'] & df_uncleaned['valid_dwell_times'] & df_uncleaned['from_geometry'].notna() & df_uncleaned['to_geometry'].notna()]\n",
    "    has_geometry_mask = df.groupby(['line', 'route', 'route_id', 'trip', 'date'])['has_geometry'].transform('all')\n",
    "    df = df.loc[has_geometry_mask]\n",
    "    df = df.copy()\n",
    "\n",
    "    if df.empty:\n",
    "        return pd.DataFrame({\n",
    "            'lau':                 [get_file_name(path)],\n",
    "            'pct_valid':           [0],\n",
    "            'line_pct_valid':     [0]\n",
    "        })\n",
    "\n",
    "    # extract year & ISOâ€‘week\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['week'] = df['date'].dt.isocalendar().week.astype(int)\n",
    "\n",
    "    def calc_cov(gp):\n",
    "        covs = []\n",
    "        for yr in (2022, 2023, 2024):\n",
    "            weeks_seen = gp.loc[gp['year']==yr, 'week'].nunique()\n",
    "            covs.append(weeks_seen / iso_weeks_in_year(yr))\n",
    "        return min(covs)  # will be 0 if any year is missing\n",
    "\n",
    "    lau_coverage = calc_cov(df[['year', 'week']].drop_duplicates())\n",
    "    \n",
    "    line_coverage = df[['line', 'year', 'week']].drop_duplicates() \\\n",
    "        .groupby(['line']) \\\n",
    "        .apply(calc_cov) \\\n",
    "        .reset_index(name='line_coverage')\n",
    "\n",
    "    link_counts = df[['line', 'route', 'route_id']].drop_duplicates()\n",
    "    link_counts['link_count'] = link_counts['route'].str.count('>') + 1\n",
    "\n",
    "    trip_counts = df[['date', 'line', 'route', 'route_id', 'trip']].drop_duplicates()\n",
    "    trip_counts = trip_counts.groupby(['line', 'route', 'route_id']) \\\n",
    "        .apply(lambda g: g.drop_duplicates(subset=['date','trip']).shape[0]) \\\n",
    "        .reset_index(name='trip_count')\n",
    "\n",
    "    route_info = line_coverage \\\n",
    "        .merge(link_counts, on=['line']) \\\n",
    "        .merge(trip_counts, on=['route', 'route_id']) \\\n",
    "\n",
    "    total_trip_count = df[['date', 'line', 'route', 'route_id', 'trip']].drop_duplicates()\n",
    "    total_trip_count = total_trip_count.drop_duplicates(subset=['line','date','trip']).shape[0]\n",
    "\n",
    "    uncleaned_total_trip_count = df_uncleaned[['date', 'line', 'route', 'route_id', 'trip']].drop_duplicates()\n",
    "    uncleaned_total_trip_count = uncleaned_total_trip_count.drop_duplicates(subset=['line','date','trip']).shape[0]\n",
    "\n",
    "    route_info['lau'] = get_file_name(path)\n",
    "    route_info['lau_coverage'] = lau_coverage\n",
    "    route_info['total_trip_count'] = total_trip_count\n",
    "    route_info['uncleaned_total_trip_count'] = uncleaned_total_trip_count\n",
    "\n",
    "    return route_info\n",
    "\n",
    "paths = glob('/mnt/nvme/sql/kv6_validated/travel_time/*.parquet')\n",
    "counts = [summarize_trips(p) for p in tqdm(paths)]\n",
    "summary_df = pd.concat(counts, ignore_index=True)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.to_parquet('summary_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.read_parquet('summary_df.parquet')\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts = pd.read_csv('/data/dev/benchmark/sampling/EU-27-LAU-2023-NUTS-2021.csv', sep=';')\n",
    "nuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine(\n",
    "    \"postgresql+psycopg2://redacted:redacted@db:5432/bison\"\n",
    ")\n",
    "\n",
    "def get_stratum(row):\n",
    "    if row['DEGURBA'] == 1 and row['POPULATION'] > 250000:\n",
    "        return '1.0'\n",
    "    elif row['DEGURBA'] == 1:\n",
    "        return '1.1'\n",
    "    else:\n",
    "        return str(row['DEGURBA'])\n",
    "nuts['DEGURBA_EXT'] = nuts.apply(get_stratum, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_has_duplicate_stops(row):\n",
    "    stops = row['route'].split('>')\n",
    "    for i in range(len(stops) - 1):\n",
    "        if stops[i] == stops[i + 1]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "dummy_stops = pd.read_csv('dummy-blacklist.csv', dtype=str)\n",
    "dummy_stops = dummy_stops['dataownercode'] + ':' + dummy_stops['userstopcode']\n",
    "\n",
    "def route_has_dummy_stop(row):\n",
    "    stops = row['route'].split('>')\n",
    "    return dummy_stops.isin(stops).any()\n",
    "\n",
    "tmp = summary_df[\n",
    "    (summary_df['lau_coverage'] >= 0.5) &\n",
    "    (summary_df['line_coverage'] >= 0.5) &\n",
    "    (summary_df['trip_count'] >= 100) &\n",
    "    (summary_df['link_count'] >= 5)\n",
    "]\n",
    "tmp = tmp[\n",
    "    ~tmp.apply(route_has_dummy_stop, axis=1)\n",
    "]\n",
    "tmp['retention_rate'] = tmp.groupby('lau')['trip_count'].transform('sum') / tmp['uncleaned_total_trip_count']\n",
    "tmp = tmp[tmp['retention_rate'] > 0.25].groupby('lau').agg({'trip_count': 'sum'})\n",
    "\n",
    "df_merged = pd.merge(\n",
    "    nuts,\n",
    "    tmp,\n",
    "    left_on='LAU CODE',\n",
    "    right_on='lau',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "sampled_laus = []\n",
    "for degurba, group in df_merged.groupby('DEGURBA_EXT'):\n",
    "    if degurba == '1.0':\n",
    "        sampled = group\n",
    "    else:\n",
    "        sampled = group.sample(5)\n",
    "    sampled = sampled.sort_values(['DEGURBA_EXT', 'POPULATION'], ascending=[True, False])\n",
    "    print(f\"Sampled {degurba} ({len(sampled)} out of {len(group)})\")\n",
    "    print(f\"Trip count: {sampled['trip_count'].sum()}\")\n",
    "    display(sampled)\n",
    "    sampled_laus.append(sampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_laus_concat = pd.concat(sampled_laus)\n",
    "sampled_laus_concat.to_csv('sampled_laus.csv', index=False)\n",
    "sampled_laus_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_laus_concat = pd.read_csv('sampled_laus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_for_output(df):\n",
    "    df['date'] = df['date'].dt.strftime('%Y-%m-%d')\n",
    "    df['from_time'] = df['from_time'].dt.strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "    df['to_time'] = df['to_time'].dt.strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "    df['valid'] = (df['valid'] & df['valid_dwell_times']).astype('int8')\n",
    "    df['route'] = df['route_id']\n",
    "    if 'from_geometry' in df.columns:\n",
    "        return df[['lau', 'date', 'line', 'trip', 'route', 'from_stop', 'to_stop', 'from_geometry', 'to_geometry', 'from_time', 'to_time']]\n",
    "    else:\n",
    "        return df[['lau', 'date', 'line', 'trip', 'route', 'stop', 'geometry', 'from_time', 'to_time']]\n",
    "\n",
    "dummy_stops = pd.read_csv('dummy-blacklist.csv', dtype=str)\n",
    "dummy_stops = dummy_stops['dataownercode'] + ':' + dummy_stops['userstopcode']\n",
    "\n",
    "def route_has_dummy_stop(row):\n",
    "    stops = row['route'].split('>')\n",
    "    return dummy_stops.isin(stops).any()\n",
    "\n",
    "for lau in sampled_laus_concat['LAU CODE'].drop_duplicates().iloc[::-1]:\n",
    "    print(f\"Processing {lau}\")\n",
    "    routes = summary_df[\n",
    "        (summary_df['lau'] == lau) &\n",
    "        (summary_df['lau_coverage'] >= 0.5) &\n",
    "        (summary_df['line_coverage'] >= 0.50) &\n",
    "        (summary_df['trip_count'] >= 100) &\n",
    "        (summary_df['link_count'] >= 5)\n",
    "    ]\n",
    "    routes = routes[~routes.apply(route_has_dummy_stop, axis=1)]\n",
    "    routes = routes[['route', 'route_id']].drop_duplicates()\n",
    "    #display(routes)\n",
    "\n",
    "    print(f\"Processing travel time for {lau}\")\n",
    "    tt = pd.read_parquet(f'/mnt/nvme/sql/kv6_validated/travel_time/{lau}.parquet')\n",
    "    tt['has_geometry'] = tt['from_geometry'].notna() & tt['to_geometry'].notna()\n",
    "    tt = tt[tt['valid'] & tt['valid_dwell_times']]\n",
    "    has_geometry_mask = tt.groupby(['line', 'route', 'route_id', 'trip', 'date'])['has_geometry'].transform('all')\n",
    "    tt = tt.loc[has_geometry_mask]\n",
    "    tt = pd.merge(\n",
    "        tt,\n",
    "        routes,\n",
    "        on=['route', 'route_id'],\n",
    "        how='inner'\n",
    "    )\n",
    "    tt = clean_for_output(tt)\n",
    "    tt.to_csv(f'/mnt/nvme/sql/kv6_validated/travel_time_filtered/{lau}.csv', index=False)\n",
    "    \n",
    "    print(f\"Processing dwell time for {lau}\")\n",
    "    dt = pd.read_parquet(f'/mnt/nvme/sql/kv6_validated/dwell_time/{lau}.parquet')\n",
    "    dt['has_geometry'] = dt['geometry'].notna()\n",
    "    # not sure if removing all the ones with no route is correct\n",
    "    dt = dt[dt['valid'] & dt['valid_dwell_times'] & dt['route_id'].notna()]\n",
    "    # does this filter out the same ones as above? probably should match trips instead?\n",
    "    has_geometry_mask = dt.groupby(['line', 'route', 'route_id', 'trip', 'date'])['has_geometry'].transform('all')\n",
    "    dt = dt.loc[has_geometry_mask]\n",
    "    dt = pd.merge(\n",
    "        dt,\n",
    "        routes,\n",
    "        on=['route', 'route_id'],\n",
    "        how='inner'\n",
    "    )\n",
    "    dt = clean_for_output(dt)\n",
    "    dt.to_csv(f'/mnt/nvme/sql/kv6_validated/dwell_time_filtered/{lau}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geopandas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
